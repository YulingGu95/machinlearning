---
title: "Machine Learning Week 4 Project"
author: "Yuling Gu"
date: "November 10, 2017"
output: html_document
---

###Introduction:  
The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

###GET DATA
```{r}
##load required packages
library(caret)
##set the working directory
setwd("C:/Users/betty/Desktop")
##import the training and testing dataset
training <- read.csv("pml-training.csv",na.strings = c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv",na.strings = c("NA","#DIV/0!",""))
```

```{r}
##get the general idea of the dataset
 dim(training)
 str(training)
```
As shown above, the training dataset contains 160 variables,and with some variables contains lots of NA values. We probably want to remove those variables with high volume of NAs, and those non relevent varaibles. I will perform analysis based on only none zero variabels.

###CLEANING DATA
```{r}
##remove those near zero variance variables
nzvtrain <- nearZeroVar(training)
training <- training[-nzvtrain]

##remove those variables that does not make sense
training <-training[,7:length(colnames(training))]

##remove those variables that contains high volumes of NAs.
nacol <- as.vector(apply(training,2,function(training) length(which(!is.na(training)))))
##remove for variables that contains (40%) of NA values.
dropnas <- c()
for (i in 1:length(nacol)) {
     if (nacol[i] > nrow(training)*.40) {
         dropnas <- c(dropnas, colnames(training)[i])
     }
 }
training <- training[,names(training) %in% dropnas]
```
Since we still have a pretty large predictors and datasets, we can break it into 3 seperate dataset to incorportate in 3 different model.
```{r}
set.seed(110)
trainsub <- createDataPartition(training$classe,p=1/3,list=FALSE)
train1 <-training[trainsub,]
temp <- training[-trainsub,]

set.seed(111)
trainsub2 <-createDataPartition(y=temp$classe,p=0.5,list=FALSE)
train2 <- temp[trainsub2,]
train3 <- temp[-trainsub2,]
dim(train1);dim(train2);dim(train3)
```
###Incorportaing with models
#####1.(gbm) Stochastic boosting trees  
#####2.(rf) random forest decision trees  
#####3.(rpart) decision trees with CART  

```{r}
set.seed(112)
mod_fit1 <- train(classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=train1)
mod_fit2 <-train(classe~.,method="rpart",preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=train2)
mod_fit3 <-train(classe~.,method="gbm",preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=train3,verbose=FALSE)

pred_rf <-predict(mod_fit1,train1)
confusionMatrix(pred_rf,train1$classe)

pred_rpart <-predict(mod_fit2,train2)
confusionMatrix(pred_rpart,train2$classe)

pred_gbm <-predict(mod_fit3,train3)
confusionMatrix(pred_gbm,train3$classe)
```
Based on those three model, random forest have the most acctuary. So I decide to accept the random forest model as the champion and move on to prediction in the testing sample.   

```{r}
print(predTest <-predict(mod_fit1,newdata=testing))
```
